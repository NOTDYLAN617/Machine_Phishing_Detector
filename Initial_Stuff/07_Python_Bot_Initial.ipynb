{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a6173a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot is running…\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply() \n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, filters\n",
    "\n",
    "\n",
    "TOKEN = \"8542599825:AAFrYe5bcguvSaRCbKf5gQ6OaTTOUK9Eagk\"\n",
    "MODEL_PATH = \"NotGood_exported_model\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def detect_phishing(text: str):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=128).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    Temperature = 2.0\n",
    "    logits = outputs.logits / Temperature\n",
    "    \n",
    "    probs = torch.softmax(logits, dim=1)[0]\n",
    "    phishing_prob = float(probs[1])\n",
    "    not_phishing_prob = float(probs[0])\n",
    "    \n",
    "    return phishing_prob, not_phishing_prob\n",
    "\n",
    "\n",
    "async def start(update, context):\n",
    "    await update.message.reply_text(\"Hello! Send me a message and I'll check for phishing.\")\n",
    "\n",
    "async def analyze(update, context):\n",
    "    text = update.message.text\n",
    "    max_len = 128\n",
    "    \n",
    "    if len(text.split()) > max_len:\n",
    "        warning = f\"\\n\\n*Warning:* The message exceeds {max_len} words. Results may be less accurate.\"\n",
    "        \n",
    "    else:\n",
    "        warning = \"\"\n",
    "    \n",
    "    phishing_prob, not_phishing_prob = detect_phishing(text)\n",
    "    \n",
    "    if phishing_prob >= 0.8:\n",
    "        verdict = \"*Very High Phishing Risk*\"\n",
    "        confidence = phishing_prob\n",
    "    \n",
    "    elif phishing_prob >= 0.4:\n",
    "        verdict = \"*Potential Phishing Risk*\"\n",
    "        confidence = phishing_prob\n",
    "        \n",
    "    else:\n",
    "        verdict = \"*No Obvious Signs of Phishing*\"\n",
    "        confidence = not_phishing_prob\n",
    "\n",
    "    \n",
    "    await update.message.reply_markdown(\n",
    "        f\"*Phishing Detection*\\n\"\n",
    "        f\"**Verdict:** {verdict}\\n\"\n",
    "        f\"**Confidence:** {confidence:.2%}\\n\"\n",
    "        f\"{warning}\"\n",
    "    )\n",
    "\n",
    "async def help_command(update, context):\n",
    "    help_message = (\n",
    "        \"**Cool Phishing Detector Bot Help**\\n\\n\"\n",
    "        \"Send or forward any text message to the bot, and it will analyze the content to determine whether it is phishing or not.\\n\\n\"\n",
    "        \"*Verdict*: The desicion of the Bot weather if the message is legitimate or phishing.\\n\\n\"\n",
    "        \"*Confidence*: The confidence of the Bot on the verdict it presented.\\n\\n\"\n",
    "        \"**Commands:**\\n\\n\"\n",
    "        \"/start - Start the bot and receive a welcome message.\\n\"\n",
    "        \"/help - Display this help message.\\n\"\n",
    "        \"/status - Get information about the model and its running device.\\n\"\n",
    "    )\n",
    "    await update.message.reply_markdown(help_message)\n",
    "    \n",
    "async def handle_media(update, context):\n",
    "    await update.message.reply_text(\"I'm a text-based analyzer! Please copy the text from the image or file and send it to me for analysis.\")\n",
    "    \n",
    "async def status_command(update, context):\n",
    "    device_status = \"GPU\" if torch.cuda.is_available() else \"CPU\"\n",
    "    await update.message.reply_text(\n",
    "        f\"Model is running on {device_status}.\"\n",
    "        f\"\\nModel path: {MODEL_PATH}\"\n",
    "        f\"\\nTokenizer: {tokenizer.__class__.__name__}\"\n",
    "        f\"\\nModel: {model.__class__.__name__}\"\n",
    "    )\n",
    "\n",
    "async def main():\n",
    "    app = ApplicationBuilder().token(TOKEN).build()\n",
    "    app.add_handler(CommandHandler(\"start\", start))\n",
    "    app.add_handler(CommandHandler(\"help\", help_command))\n",
    "    app.add_handler(MessageHandler(filters.PHOTO | filters.ATTACHMENT, handle_media))\n",
    "    app.add_handler(CommandHandler(\"status\", status_command))\n",
    "    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, analyze))\n",
    "    print(\"Bot is running…\")\n",
    "    await app.run_polling()\n",
    "\n",
    "\n",
    "await main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
